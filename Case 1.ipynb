{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e2423",
   "metadata": {},
   "source": [
    "#### Case Study #1: Claim Resubmission Ingestion Pipeline\n",
    "\n",
    "- Author: Daniel Dabiri\n",
    "- Date: 2025\n",
    "- Description: Robust pipeline for ingesting, normalizing, and analyzing insurance claim data from multiple EMR systems claims eligible for resubmission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964e41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danieldabiri/Library/Python/3.12/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danieldabiri/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.3.2 pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be76c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS AND SETUP\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from abc import ABC, abstractmethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12738423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging with detailed format\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('claim_pipeline.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437dee09",
   "metadata": {},
   "source": [
    "##### ============================================================================\n",
    "\n",
    "##### CONFIGURATION AND CONSTANTS\n",
    "\n",
    "##### ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6001966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Central configuration for the pipeline\"\"\"\n",
    "    REFERENCE_DATE = datetime(2025, 7, 30)  # Today's date for eligibility calculation\n",
    "    DAYS_THRESHOLD = 7  # Claims must be older than this many days\n",
    "    \n",
    "    # Denial reason categories\n",
    "    RETRYABLE_REASONS = {\n",
    "        \"missing modifier\",\n",
    "        \"incorrect npi\",\n",
    "        \"prior auth required\"\n",
    "    }\n",
    "    \n",
    "    NON_RETRYABLE_REASONS = {\n",
    "        \"authorization expired\",\n",
    "        \"incorrect provider type\"\n",
    "    }\n",
    "    \n",
    "    # Ambiguous reasons that need classification\n",
    "    AMBIGUOUS_MAPPING = {\n",
    "        \"incorrect procedure\": False,  # Not retryable based on business logic\n",
    "        \"form incomplete\": True,        # Retryable - can complete form\n",
    "        \"not billable\": False,          # Not retryable\n",
    "        None: False,                    # Null reasons not retryable by default\n",
    "        \"\": False                       # Empty strings not retryable\n",
    "    }\n",
    "\n",
    "class ClaimStatus(Enum):\n",
    "    \"\"\"Enum for claim statuses\"\"\"\n",
    "    APPROVED = \"approved\"\n",
    "    DENIED = \"denied\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb77d0",
   "metadata": {},
   "source": [
    "##### ============================================================================\n",
    "\n",
    "##### DATA MODEL\n",
    "\n",
    "##### ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7af310",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UnifiedClaim:\n",
    "    \"\"\"\n",
    "    Unified schema for claims from all sources.\n",
    "    This ensures consistent data structure across different EMR systems.\n",
    "    \"\"\"\n",
    "    claim_id: str\n",
    "    patient_id: Optional[str]\n",
    "    procedure_code: str\n",
    "    denial_reason: Optional[str]\n",
    "    status: str\n",
    "    submitted_at: str  # ISO format date\n",
    "    source_system: str\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        return asdict(self)\n",
    "    \n",
    "    def is_valid_for_resubmission(self) -> bool:\n",
    "        \"\"\"Check if claim has minimum required fields for resubmission\"\"\"\n",
    "        return bool(self.claim_id and self.patient_id and self.status)\n",
    "\n",
    "@dataclass\n",
    "class ResubmissionCandidate:\n",
    "    \"\"\"Model for claims eligible for resubmission\"\"\"\n",
    "    claim_id: str\n",
    "    resubmission_reason: str\n",
    "    source_system: str\n",
    "    recommended_changes: str\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        return asdict(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802a907",
   "metadata": {},
   "source": [
    "##### ============================================================================\n",
    "\n",
    "##### DATA SOURCE HANDLERS\n",
    "\n",
    "##### ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1b702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSourceHandler(ABC):\n",
    "    \"\"\"Abstract base class for data source handlers\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_data(self, filepath: str) -> List[Dict]:\n",
    "        \"\"\"Load data from source file\"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def normalize(self, data: List[Dict]) -> List[UnifiedClaim]:\n",
    "        \"\"\"Normalize data to unified schema\"\"\"\n",
    "        pass\n",
    "\n",
    "class EMRAlphaHandler(DataSourceHandler):\n",
    "    \"\"\"Handler for EMR Alpha CSV data source\"\"\"\n",
    "    \n",
    "    def load_data(self, filepath: str) -> List[Dict]:\n",
    "        \"\"\"Load CSV data with robust error handling\"\"\"\n",
    "        logger.info(f\"Loading EMR Alpha data from {filepath}\")\n",
    "        data = []\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row_num, row in enumerate(reader, start=2):  # Start at 2 (header is line 1)\n",
    "                    try:\n",
    "                        data.append(row)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error processing row {row_num}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            logger.info(f\"Successfully loaded {len(data)} records from EMR Alpha\")\n",
    "            return data\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {filepath}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading EMR Alpha data: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def normalize(self, data: List[Dict]) -> List[UnifiedClaim]:\n",
    "        \"\"\"Normalize CSV data to unified schema\"\"\"\n",
    "        logger.info(\"Normalizing EMR Alpha data\")\n",
    "        normalized = []\n",
    "        \n",
    "        for record in data:\n",
    "            try:\n",
    "                # Handle missing or empty patient_id\n",
    "                patient_id = record.get('patient_id', '').strip()\n",
    "                patient_id = patient_id if patient_id else None\n",
    "                \n",
    "                # Normalize denial reason (lowercase, handle 'None' string)\n",
    "                denial_reason = record.get('denial_reason', '').strip()\n",
    "                if denial_reason.lower() == 'none':\n",
    "                    denial_reason = None\n",
    "                elif denial_reason:\n",
    "                    denial_reason = denial_reason.lower()\n",
    "                else:\n",
    "                    denial_reason = None\n",
    "                \n",
    "                # Parse and normalize date\n",
    "                submitted_date = self._parse_date(record.get('submitted_at', ''))\n",
    "                \n",
    "                claim = UnifiedClaim(\n",
    "                    claim_id=record.get('claim_id', '').strip(),\n",
    "                    patient_id=patient_id,\n",
    "                    procedure_code=record.get('procedure_code', '').strip(),\n",
    "                    denial_reason=denial_reason,\n",
    "                    status=record.get('status', '').lower().strip(),\n",
    "                    submitted_at=submitted_date,\n",
    "                    source_system=\"alpha\"\n",
    "                )\n",
    "                \n",
    "                if claim.claim_id:  # Only add if claim has an ID\n",
    "                    normalized.append(claim)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping record with missing claim_id: {record}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error normalizing record {record}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Normalized {len(normalized)} EMR Alpha records\")\n",
    "        return normalized\n",
    "    \n",
    "    def _parse_date(self, date_str: str) -> str:\n",
    "        \"\"\"Parse date to ISO format\"\"\"\n",
    "        if not date_str:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # Try parsing YYYY-MM-DD format\n",
    "            dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            return dt.isoformat()\n",
    "        except:\n",
    "            try:\n",
    "                # Try other common formats\n",
    "                dt = datetime.strptime(date_str, \"%m/%d/%Y\")\n",
    "                return dt.isoformat()\n",
    "            except:\n",
    "                logger.warning(f\"Could not parse date: {date_str}\")\n",
    "                return date_str\n",
    "\n",
    "class EMRBetaHandler(DataSourceHandler):\n",
    "    \"\"\"Handler for EMR Beta JSON data source\"\"\"\n",
    "    \n",
    "    def load_data(self, filepath: str) -> List[Dict]:\n",
    "        \"\"\"Load JSON data with robust error handling\"\"\"\n",
    "        logger.info(f\"Loading EMR Beta data from {filepath}\")\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "            if not isinstance(data, list):\n",
    "                logger.error(\"EMR Beta data is not a list\")\n",
    "                return []\n",
    "                \n",
    "            logger.info(f\"Successfully loaded {len(data)} records from EMR Beta\")\n",
    "            return data\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {filepath}\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Invalid JSON in EMR Beta file: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading EMR Beta data: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def normalize(self, data: List[Dict]) -> List[UnifiedClaim]:\n",
    "        \"\"\"Normalize JSON data to unified schema\"\"\"\n",
    "        logger.info(\"Normalizing EMR Beta data\")\n",
    "        normalized = []\n",
    "        \n",
    "        for record in data:\n",
    "            try:\n",
    "                # Map Beta field names to unified schema\n",
    "                patient_id = record.get('member')\n",
    "                if patient_id == \"\" or patient_id is None:\n",
    "                    patient_id = None\n",
    "                \n",
    "                # Normalize denial reason\n",
    "                denial_reason = record.get('error_msg')\n",
    "                if denial_reason:\n",
    "                    denial_reason = denial_reason.lower().strip()\n",
    "                \n",
    "                # Parse and normalize date (already in ISO format)\n",
    "                submitted_date = record.get('date', '')\n",
    "                if 'T' in submitted_date:\n",
    "                    # Already in ISO format, just ensure it's complete\n",
    "                    submitted_date = submitted_date\n",
    "                \n",
    "                claim = UnifiedClaim(\n",
    "                    claim_id=record.get('id', '').strip(),\n",
    "                    patient_id=patient_id,\n",
    "                    procedure_code=record.get('code', '').strip(),\n",
    "                    denial_reason=denial_reason,\n",
    "                    status=record.get('status', '').lower().strip(),\n",
    "                    submitted_at=submitted_date,\n",
    "                    source_system=\"beta\"\n",
    "                )\n",
    "                \n",
    "                if claim.claim_id:  # Only add if claim has an ID\n",
    "                    normalized.append(claim)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping record with missing claim_id: {record}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error normalizing record {record}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Normalized {len(normalized)} EMR Beta records\")\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b52f3",
   "metadata": {},
   "source": [
    "##### ============================================================================\n",
    "\n",
    "##### DENIAL REASON CLASSIFIER\n",
    "\n",
    "##### ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b273ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenialReasonClassifier:\n",
    "    \"\"\"\n",
    "    Classifier for determining if denial reasons are retryable.\n",
    "    This simulates an LLM classifier with rule-based logic and heuristics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classification_cache = {}\n",
    "        logger.info(\"Initialized DenialReasonClassifier\")\n",
    "    \n",
    "    def is_retryable(self, denial_reason: Optional[str]) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Determine if a denial reason is retryable.\n",
    "        Returns: (is_retryable, classification_method)\n",
    "        \"\"\"\n",
    "        if denial_reason is None or denial_reason == \"\":\n",
    "            return False, \"null_or_empty\"\n",
    "        \n",
    "        # Normalize the reason\n",
    "        normalized_reason = denial_reason.lower().strip()\n",
    "        \n",
    "        # Check cache first\n",
    "        if normalized_reason in self.classification_cache:\n",
    "            return self.classification_cache[normalized_reason], \"cached\"\n",
    "        \n",
    "        # Check known retryable reasons\n",
    "        if normalized_reason in Config.RETRYABLE_REASONS:\n",
    "            result = (True, \"known_retryable\")\n",
    "        # Check known non-retryable reasons\n",
    "        elif normalized_reason in Config.NON_RETRYABLE_REASONS:\n",
    "            result = (False, \"known_non_retryable\")\n",
    "        # Check ambiguous mapping\n",
    "        elif normalized_reason in Config.AMBIGUOUS_MAPPING:\n",
    "            is_retry = Config.AMBIGUOUS_MAPPING[normalized_reason]\n",
    "            result = (is_retry, \"ambiguous_mapped\")\n",
    "        else:\n",
    "            # Use heuristic classifier for unknown reasons\n",
    "            result = (self._heuristic_classify(normalized_reason), \"heuristic\")\n",
    "        \n",
    "        # Cache the result\n",
    "        self.classification_cache[normalized_reason] = result[0]\n",
    "        logger.debug(f\"Classified '{denial_reason}' as retryable={result[0]} using {result[1]}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _heuristic_classify(self, reason: str) -> bool:\n",
    "        \"\"\"\n",
    "        Heuristic classification for unknown denial reasons.\n",
    "        This simulates an LLM decision based on keywords and patterns.\n",
    "        \"\"\"\n",
    "        # Keywords that suggest retryable issues\n",
    "        retryable_keywords = [\n",
    "            'missing', 'incomplete', 'required', 'update', 'resubmit',\n",
    "            'incorrect format', 'needs', 'add', 'provide'\n",
    "        ]\n",
    "        \n",
    "        # Keywords that suggest non-retryable issues\n",
    "        non_retryable_keywords = [\n",
    "            'expired', 'invalid', 'not covered', 'excluded', 'terminated',\n",
    "            'not eligible', 'duplicate', 'already processed'\n",
    "        ]\n",
    "        \n",
    "        reason_lower = reason.lower()\n",
    "        \n",
    "        # Check for non-retryable keywords first (more definitive)\n",
    "        for keyword in non_retryable_keywords:\n",
    "            if keyword in reason_lower:\n",
    "                return False\n",
    "        \n",
    "        # Check for retryable keywords\n",
    "        for keyword in retryable_keywords:\n",
    "            if keyword in reason_lower:\n",
    "                return True\n",
    "        \n",
    "        # Default to non-retryable for unknown patterns\n",
    "        return False\n",
    "    \n",
    "    def get_recommendation(self, denial_reason: str) -> str:\n",
    "        \"\"\"Get recommended changes for a denial reason\"\"\"\n",
    "        if denial_reason is None:\n",
    "            return \"Review claim for completeness\"\n",
    "        \n",
    "        reason_lower = denial_reason.lower()\n",
    "        \n",
    "        recommendations = {\n",
    "            \"missing modifier\": \"Add appropriate modifier codes to the claim\",\n",
    "            \"incorrect npi\": \"Review NPI number and resubmit\",\n",
    "            \"prior auth required\": \"Obtain prior authorization and resubmit\",\n",
    "            \"form incomplete\": \"Complete all required form fields\",\n",
    "            \"incorrect procedure\": \"Verify procedure code accuracy\",\n",
    "        }\n",
    "        \n",
    "        return recommendations.get(reason_lower, f\"Address issue: {denial_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bde37f",
   "metadata": {},
   "source": [
    "##### ============================================================================\n",
    "\n",
    "##### RESUBMISSION ELIGIBILITY ENGINE\n",
    "\n",
    "##### ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2cc8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResubmissionEligibilityEngine:\n",
    "    \"\"\"Engine for determining claim resubmission eligibility\"\"\"\n",
    "    \n",
    "    def __init__(self, classifier: DenialReasonClassifier):\n",
    "        self.classifier = classifier\n",
    "        self.stats = {\n",
    "            'total_processed': 0,\n",
    "            'denied_claims': 0,\n",
    "            'missing_patient_id': 0,\n",
    "            'too_recent': 0,\n",
    "            'non_retryable_reason': 0,\n",
    "            'eligible': 0\n",
    "        }\n",
    "        logger.info(\"Initialized ResubmissionEligibilityEngine\")\n",
    "    \n",
    "    def evaluate_claims(self, claims: List[UnifiedClaim]) -> List[ResubmissionCandidate]:\n",
    "        \"\"\"Evaluate claims for resubmission eligibility\"\"\"\n",
    "        logger.info(f\"Evaluating {len(claims)} claims for resubmission eligibility\")\n",
    "        candidates = []\n",
    "        \n",
    "        for claim in claims:\n",
    "            self.stats['total_processed'] += 1\n",
    "            \n",
    "            # Check eligibility criteria\n",
    "            if self._is_eligible(claim):\n",
    "                candidate = ResubmissionCandidate(\n",
    "                    claim_id=claim.claim_id,\n",
    "                    resubmission_reason=claim.denial_reason or \"Unknown\",\n",
    "                    source_system=claim.source_system,\n",
    "                    recommended_changes=self.classifier.get_recommendation(claim.denial_reason or \"\")\n",
    "                )\n",
    "                candidates.append(candidate)\n",
    "                self.stats['eligible'] += 1\n",
    "                logger.debug(f\"Claim {claim.claim_id} is eligible for resubmission\")\n",
    "        \n",
    "        logger.info(f\"Found {len(candidates)} eligible claims for resubmission\")\n",
    "        return candidates\n",
    "    \n",
    "    def _is_eligible(self, claim: UnifiedClaim) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a claim is eligible for resubmission.\n",
    "        \n",
    "        Criteria:\n",
    "        1. Status is denied\n",
    "        2. Patient ID is not null\n",
    "        3. Claim was submitted more than 7 days ago\n",
    "        4. Denial reason is retryable\n",
    "        \"\"\"\n",
    "        # Criterion 1: Status must be denied\n",
    "        if claim.status != ClaimStatus.DENIED.value:\n",
    "            return False\n",
    "        \n",
    "        self.stats['denied_claims'] += 1\n",
    "        \n",
    "        # Criterion 2: Patient ID must not be null\n",
    "        if not claim.patient_id:\n",
    "            self.stats['missing_patient_id'] += 1\n",
    "            logger.debug(f\"Claim {claim.claim_id} missing patient_id\")\n",
    "            return False\n",
    "        \n",
    "        # Criterion 3: Claim must be older than threshold\n",
    "        try:\n",
    "            submitted_date = datetime.fromisoformat(claim.submitted_at.replace('Z', '+00:00'))\n",
    "            days_old = (Config.REFERENCE_DATE - submitted_date.replace(tzinfo=None)).days\n",
    "            \n",
    "            if days_old <= Config.DAYS_THRESHOLD:\n",
    "                self.stats['too_recent'] += 1\n",
    "                logger.debug(f\"Claim {claim.claim_id} too recent ({days_old} days old)\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not parse date for claim {claim.claim_id}: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Criterion 4: Denial reason must be retryable\n",
    "        is_retryable, method = self.classifier.is_retryable(claim.denial_reason)\n",
    "        if not is_retryable:\n",
    "            self.stats['non_retryable_reason'] += 1\n",
    "            logger.debug(f\"Claim {claim.claim_id} has non-retryable reason: {claim.denial_reason}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get processing statistics\"\"\"\n",
    "        return self.stats.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce91b2",
   "metadata": {},
   "source": [
    "##### ============================================================================\n",
    "\n",
    "##### MAIN PIPELINE ORCHESTRATOR\n",
    "\n",
    "##### ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a6520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimResubmissionPipeline:\n",
    "    \"\"\"Main pipeline orchestrator for claim resubmission processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alpha_handler = EMRAlphaHandler()\n",
    "        self.beta_handler = EMRBetaHandler()\n",
    "        self.classifier = DenialReasonClassifier()\n",
    "        self.eligibility_engine = ResubmissionEligibilityEngine(self.classifier)\n",
    "        self.metrics = {\n",
    "            'source_counts': {},\n",
    "            'total_claims': 0,\n",
    "            'eligible_claims': 0,\n",
    "            'processing_errors': 0\n",
    "        }\n",
    "        logger.info(\"Initialized ClaimResubmissionPipeline\")\n",
    "    \n",
    "    def run(self, alpha_file: str, beta_file: str, output_file: str = \"resubmission_candidates.json\"):\n",
    "        \"\"\"\n",
    "        Execute the complete pipeline.\n",
    "        \n",
    "        Args:\n",
    "            alpha_file: Path to EMR Alpha CSV file\n",
    "            beta_file: Path to EMR Beta JSON file\n",
    "            output_file: Path for output JSON file\n",
    "        \"\"\"\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Starting Claim Resubmission Pipeline\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load and normalize data from all sources\n",
    "            all_claims = self._load_and_normalize_data(alpha_file, beta_file)\n",
    "            \n",
    "            if not all_claims:\n",
    "                logger.error(\"No claims loaded from any source\")\n",
    "                return\n",
    "            \n",
    "            # Step 2: Evaluate claims for resubmission eligibility\n",
    "            candidates = self.eligibility_engine.evaluate_claims(all_claims)\n",
    "            \n",
    "            # Step 3: Save results\n",
    "            self._save_results(candidates, output_file)\n",
    "            \n",
    "            # Step 4: Generate and display metrics\n",
    "            self._generate_metrics(candidates)\n",
    "            \n",
    "            # Step 5: Create rejection log\n",
    "            self._create_rejection_log(all_claims, candidates)\n",
    "            \n",
    "            logger.info(\"=\" * 60)\n",
    "            logger.info(\"Pipeline completed successfully\")\n",
    "            logger.info(\"=\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {e}\")\n",
    "            self.metrics['processing_errors'] += 1\n",
    "            raise\n",
    "    \n",
    "    def _load_and_normalize_data(self, alpha_file: str, beta_file: str) -> List[UnifiedClaim]:\n",
    "        \"\"\"Load and normalize data from all sources\"\"\"\n",
    "        all_claims = []\n",
    "        \n",
    "        # Process EMR Alpha\n",
    "        logger.info(\"Processing EMR Alpha source...\")\n",
    "        alpha_data = self.alpha_handler.load_data(alpha_file)\n",
    "        alpha_claims = self.alpha_handler.normalize(alpha_data)\n",
    "        all_claims.extend(alpha_claims)\n",
    "        self.metrics['source_counts']['alpha'] = len(alpha_claims)\n",
    "        \n",
    "        # Process EMR Beta\n",
    "        logger.info(\"Processing EMR Beta source...\")\n",
    "        beta_data = self.beta_handler.load_data(beta_file)\n",
    "        beta_claims = self.beta_handler.normalize(beta_data)\n",
    "        all_claims.extend(beta_claims)\n",
    "        self.metrics['source_counts']['beta'] = len(beta_claims)\n",
    "        \n",
    "        self.metrics['total_claims'] = len(all_claims)\n",
    "        logger.info(f\"Total claims loaded: {len(all_claims)}\")\n",
    "        \n",
    "        return all_claims\n",
    "    \n",
    "    def _save_results(self, candidates: List[ResubmissionCandidate], output_file: str):\n",
    "        \"\"\"Save resubmission candidates to JSON file\"\"\"\n",
    "        logger.info(f\"Saving {len(candidates)} candidates to {output_file}\")\n",
    "        \n",
    "        try:\n",
    "            output_data = [candidate.to_dict() for candidate in candidates]\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"Successfully saved results to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving results: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _generate_metrics(self, candidates: List[ResubmissionCandidate]):\n",
    "        \"\"\"Generate and display processing metrics\"\"\"\n",
    "        self.metrics['eligible_claims'] = len(candidates)\n",
    "        eligibility_stats = self.eligibility_engine.get_statistics()\n",
    "        \n",
    "        # Print comprehensive metrics\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"PIPELINE METRICS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\n📊 Data Sources:\")\n",
    "        for source, count in self.metrics['source_counts'].items():\n",
    "            print(f\"  - EMR {source.capitalize()}: {count} claims\")\n",
    "        \n",
    "        print(f\"\\n📈 Processing Summary:\")\n",
    "        print(f\"  - Total claims processed: {self.metrics['total_claims']}\")\n",
    "        print(f\"  - Denied claims: {eligibility_stats['denied_claims']}\")\n",
    "        print(f\"  - Eligible for resubmission: {self.metrics['eligible_claims']}\")\n",
    "        \n",
    "        print(f\"\\n❌ Exclusion Reasons:\")\n",
    "        print(f\"  - Missing patient ID: {eligibility_stats['missing_patient_id']}\")\n",
    "        print(f\"  - Too recent (≤7 days): {eligibility_stats['too_recent']}\")\n",
    "        print(f\"  - Non-retryable denial reason: {eligibility_stats['non_retryable_reason']}\")\n",
    "        \n",
    "        if self.metrics['eligible_claims'] > 0:\n",
    "            print(f\"\\n✅ Resubmission Rate: {self.metrics['eligible_claims']/self.metrics['total_claims']*100:.1f}%\")\n",
    "        \n",
    "        # Save metrics to file\n",
    "        with open('pipeline_metrics.json', 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        \n",
    "        logger.info(\"Metrics saved to pipeline_metrics.json\")\n",
    "    \n",
    "    def _create_rejection_log(self, all_claims: List[UnifiedClaim], candidates: List[ResubmissionCandidate]):\n",
    "        \"\"\"Create a log of rejected claims\"\"\"\n",
    "        eligible_ids = {c.claim_id for c in candidates}\n",
    "        rejected_claims = [\n",
    "            claim.to_dict() for claim in all_claims \n",
    "            if claim.claim_id not in eligible_ids and claim.status == 'denied'\n",
    "        ]\n",
    "        \n",
    "        if rejected_claims:\n",
    "            with open('rejected_claims.json', 'w') as f:\n",
    "                json.dump(rejected_claims, f, indent=2)\n",
    "            logger.info(f\"Saved {len(rejected_claims)} rejected claims to rejected_claims.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a440dc",
   "metadata": {},
   "source": [
    "##### ============================================================================\n",
    "\n",
    "##### SAMPLE DATA CREATION (for testing)\n",
    "\n",
    "##### ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a675743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data():\n",
    "    \"\"\"Create sample data files for testing the pipeline\"\"\"\n",
    "    \n",
    "    # Create EMR Alpha CSV\n",
    "    alpha_data = \"\"\"claim_id,patient_id,procedure_code,denial_reason,submitted_at,status\n",
    "A123,P001,99213,Missing modifier,2025-07-01,denied\n",
    "A124,P002,99214,Incorrect NPI,2025-07-10,denied\n",
    "A125,,99215,Authorization expired,2025-07-05,denied\n",
    "A126,P003,99381,None,2025-07-15,approved\n",
    "A127,P004,99401,Prior auth required,2025-07-20,denied\"\"\"\n",
    "    \n",
    "    with open('emr_alpha.csv', 'w') as f:\n",
    "        f.write(alpha_data)\n",
    "    \n",
    "    # Create EMR Beta JSON\n",
    "    beta_data = [\n",
    "        {\n",
    "            \"id\": \"B987\",\n",
    "            \"member\": \"P010\",\n",
    "            \"code\": \"99213\",\n",
    "            \"error_msg\": \"Incorrect provider type\",\n",
    "            \"date\": \"2025-07-03T00:00:00\",\n",
    "            \"status\": \"denied\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"B988\",\n",
    "            \"member\": \"P011\",\n",
    "            \"code\": \"99214\",\n",
    "            \"error_msg\": \"Missing modifier\",\n",
    "            \"date\": \"2025-07-09T00:00:00\",\n",
    "            \"status\": \"denied\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"B989\",\n",
    "            \"member\": \"P012\",\n",
    "            \"code\": \"99215\",\n",
    "            \"error_msg\": None,\n",
    "            \"date\": \"2025-07-10T00:00:00\",\n",
    "            \"status\": \"approved\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"B990\",\n",
    "            \"member\": None,\n",
    "            \"code\": \"99401\",\n",
    "            \"error_msg\": \"incorrect procedure\",\n",
    "            \"date\": \"2025-07-01T00:00:00\",\n",
    "            \"status\": \"denied\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    with open('emr_beta.json', 'w') as f:\n",
    "        json.dump(beta_data, f, indent=2)\n",
    "    \n",
    "    print(\"Sample data files created: emr_alpha.csv and emr_beta.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708ce7f",
   "metadata": {},
   "source": [
    "##### ============================================================================\n",
    "\n",
    "##### MAIN EXECUTION\n",
    "\n",
    "##### ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3beffbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data files created: emr_alpha.csv and emr_beta.json\n",
      "2025-08-18 21:03:55,350 - __main__ - INFO - Initialized DenialReasonClassifier\n",
      "2025-08-18 21:03:55,358 - __main__ - INFO - Initialized ResubmissionEligibilityEngine\n",
      "2025-08-18 21:03:55,360 - __main__ - INFO - Initialized ClaimResubmissionPipeline\n",
      "2025-08-18 21:03:55,363 - __main__ - INFO - ============================================================\n",
      "2025-08-18 21:03:55,364 - __main__ - INFO - Starting Claim Resubmission Pipeline\n",
      "2025-08-18 21:03:55,365 - __main__ - INFO - ============================================================\n",
      "2025-08-18 21:03:55,367 - __main__ - INFO - Processing EMR Alpha source...\n",
      "2025-08-18 21:03:55,371 - __main__ - INFO - Loading EMR Alpha data from emr_alpha.csv\n",
      "2025-08-18 21:03:55,374 - __main__ - INFO - Successfully loaded 5 records from EMR Alpha\n",
      "2025-08-18 21:03:55,375 - __main__ - INFO - Normalizing EMR Alpha data\n",
      "2025-08-18 21:03:55,378 - __main__ - INFO - Normalized 5 EMR Alpha records\n",
      "2025-08-18 21:03:55,384 - __main__ - INFO - Processing EMR Beta source...\n",
      "2025-08-18 21:03:55,385 - __main__ - INFO - Loading EMR Beta data from emr_beta.json\n",
      "2025-08-18 21:03:55,391 - __main__ - INFO - Successfully loaded 4 records from EMR Beta\n",
      "2025-08-18 21:03:55,392 - __main__ - INFO - Normalizing EMR Beta data\n",
      "2025-08-18 21:03:55,392 - __main__ - INFO - Normalized 4 EMR Beta records\n",
      "2025-08-18 21:03:55,395 - __main__ - INFO - Total claims loaded: 9\n",
      "2025-08-18 21:03:55,395 - __main__ - INFO - Evaluating 9 claims for resubmission eligibility\n",
      "2025-08-18 21:03:55,397 - __main__ - INFO - Found 4 eligible claims for resubmission\n",
      "2025-08-18 21:03:55,399 - __main__ - INFO - Saving 4 candidates to resubmission_candidates.json\n",
      "2025-08-18 21:03:55,405 - __main__ - INFO - Successfully saved results to resubmission_candidates.json\n",
      "\n",
      "============================================================\n",
      "PIPELINE METRICS\n",
      "============================================================\n",
      "\n",
      "📊 Data Sources:\n",
      "  - EMR Alpha: 5 claims\n",
      "  - EMR Beta: 4 claims\n",
      "\n",
      "📈 Processing Summary:\n",
      "  - Total claims processed: 9\n",
      "  - Denied claims: 7\n",
      "  - Eligible for resubmission: 4\n",
      "\n",
      "❌ Exclusion Reasons:\n",
      "  - Missing patient ID: 2\n",
      "  - Too recent (≤7 days): 0\n",
      "  - Non-retryable denial reason: 1\n",
      "\n",
      "✅ Resubmission Rate: 44.4%\n",
      "2025-08-18 21:03:55,411 - __main__ - INFO - Metrics saved to pipeline_metrics.json\n",
      "2025-08-18 21:03:55,413 - __main__ - INFO - Saved 3 rejected claims to rejected_claims.json\n",
      "2025-08-18 21:03:55,414 - __main__ - INFO - ============================================================\n",
      "2025-08-18 21:03:55,415 - __main__ - INFO - Pipeline completed successfully\n",
      "2025-08-18 21:03:55,416 - __main__ - INFO - ============================================================\n",
      "\n",
      "============================================================\n",
      "RESUBMISSION CANDIDATES\n",
      "============================================================\n",
      "[\n",
      "  {\n",
      "    \"claim_id\": \"A123\",\n",
      "    \"resubmission_reason\": \"missing modifier\",\n",
      "    \"source_system\": \"alpha\",\n",
      "    \"recommended_changes\": \"Add appropriate modifier codes to the claim\"\n",
      "  },\n",
      "  {\n",
      "    \"claim_id\": \"A124\",\n",
      "    \"resubmission_reason\": \"incorrect npi\",\n",
      "    \"source_system\": \"alpha\",\n",
      "    \"recommended_changes\": \"Review NPI number and resubmit\"\n",
      "  },\n",
      "  {\n",
      "    \"claim_id\": \"A127\",\n",
      "    \"resubmission_reason\": \"prior auth required\",\n",
      "    \"source_system\": \"alpha\",\n",
      "    \"recommended_changes\": \"Obtain prior authorization and resubmit\"\n",
      "  },\n",
      "  {\n",
      "    \"claim_id\": \"B988\",\n",
      "    \"resubmission_reason\": \"missing modifier\",\n",
      "    \"source_system\": \"beta\",\n",
      "    \"recommended_changes\": \"Add appropriate modifier codes to the claim\"\n",
      "  }\n",
      "]\n",
      "\n",
      "✅ Pipeline execution completed!\n",
      "📁 Output files created:\n",
      "  - resubmission_candidates.json (eligible claims)\n",
      "  - rejected_claims.json (non-eligible denied claims)\n",
      "  - pipeline_metrics.json (processing statistics)\n",
      "  - claim_pipeline.log (detailed logs)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create sample data files\n",
    "    create_sample_data()\n",
    "    \n",
    "    # Initialize and run the pipeline\n",
    "    pipeline = ClaimResubmissionPipeline()\n",
    "    \n",
    "    # Run with sample data\n",
    "    pipeline.run(\n",
    "        alpha_file='emr_alpha.csv',\n",
    "        beta_file='emr_beta.json',\n",
    "        output_file='resubmission_candidates.json'\n",
    "    )\n",
    "    \n",
    "    # Display the output\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RESUBMISSION CANDIDATES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open('resubmission_candidates.json', 'r') as f:\n",
    "        candidates = json.load(f)\n",
    "        print(json.dumps(candidates, indent=2))\n",
    "    \n",
    "    print(f\"\\n✅ Pipeline execution completed!\")\n",
    "    print(f\"📁 Output files created:\")\n",
    "    print(f\"  - resubmission_candidates.json (eligible claims)\")\n",
    "    print(f\"  - rejected_claims.json (non-eligible denied claims)\")\n",
    "    print(f\"  - pipeline_metrics.json (processing statistics)\")\n",
    "    print(f\"  - claim_pipeline.log (detailed logs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57b2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
